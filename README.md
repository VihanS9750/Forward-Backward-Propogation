# Forward-Backward-Propogation
Implemented a multi-layer neural network from scratch using NumPy with forward &amp; backward propagation, supporting Sigmoid, ReLU, Tanh. Trained on classification data using MSE &amp; Cross-Entropy loss. Includes 70-15-15 split, performance metrics, and training loss visualization.
An end-to-end implementation of a multi-layer neural network built from scratch using NumPy. This project demonstrates forward and backward propagation, supports multiple hidden layers, and integrates Sigmoid, ReLU, and Tanh activation functions. The model is trained and evaluated on classification datasets using MSE and Cross-Entropy loss functions. Includes visualization of training loss curves and performance metrics like accuracy, precision, recall, and F1-score.
 Key Features:
1.Custom implementation of forward & backward propagation (FP & BP)
2.Support for configurable N-layer architecture
3.Activation functions: Sigmoid, ReLU, Tanh
4.Loss functions: Mean Squared Error (MSE) & Cross-Entropy
5.Train-validation-test split (70-15-15)
6.Evaluation metrics: confusion matrix, precision, recall, F1-score
7.Plotting training loss over epochs
8.No external ML libraries (pure NumPy-based)
