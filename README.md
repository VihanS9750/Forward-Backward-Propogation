# Forward-Backward-Propogation
Implemented a multi-layer neural network from scratch using NumPy with forward &amp; backward propagation, supporting Sigmoid, ReLU, Tanh. Trained on classification data using MSE &amp; Cross-Entropy loss. Includes 70-15-15 split, performance metrics, and training loss visualization.
